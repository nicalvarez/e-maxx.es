\h1{Task Joseph}

Statement of the problem. Given a natural $n$ and $k$. Around the circle write all natural numbers from 1 to $n$. First count of $k$-th number, starting with the first, and remove it. Then count the $k$ numbers and $k$-th delete, etc. the Process stops when there is only one number. You want to find this number.

The problem was posed in \bf{Joseph Flavius} (Flavius Josephus) in the 1st century (though in a somewhat more narrow formulation: if $k = 2$).

To solve this problem by simulation. The simplest simulation will run $O (n^2)$. Using \algohref=segment_tree{Tree}, it is possible to produce the modeling is $O (n \log n)$.

\h2{the Solution is $O(n)$}

Try to find a pattern, Express the answer to problem $J_{n,k}$ using the solution of previous tasks.

Through simulation we will construct a table of values, like this:

$$ \bordermatrix {
n \setminus k&1&2&3&4&5&6&7&8&9&10 \cr
1&1&1&1&1&1&1&1&1&1&1& \cr
2&2&1&2&1&2&1&2&1&2&1& \cr
3&3&3&2&2&1&1&3&3&2&2& \cr
4&4&1&1&2&2&3&2&3&3&4& \cr
5&5&3&4&1&2&4&4&1&2&4& \cr
6&6&5&1&5&1&4&5&3&5&2& \cr
7&7&7&4&2&6&3&5&4&7&5& \cr
8&8&1&7&6&3&1&4&4&8&7& \cr
9&9&3&1&1&8&7&2&3&8&8& \cr
10&10&5&4&5&3&3&9&1&7&8& \cr
} $$

And here quite clearly visible next \bf{pattern}:
$$ J_{n,k} = \left( J_{(n-1),k} + k - 1 \right)\ \%\ n + 1 $$
$$ J_{1,k} = 1 $$

Here 1-based spoils the elegance of the formula, if the number of positions with zero, you get a very clear formula:

$$ J_{n,k} = \left( J_{(n-1),k} + k \right)\ \%\ n = \sum_{i=1}^n k\ \%\ i $$

So, we have found a solution to the problem of Joseph working for $O (n)$ operations.

Simple \bf{recursive implementation} (1-indexed):
\code
int joseph (int n, int k) {
return n>1 ? (joseph (n-1, k) + k - 1) % n + 1 : 1;
}
\endcode

\bf{non-Recursive form}:
\code
int joseph (int n, int k) {
int res = 0;
for (int i=1; i<=n; ++i)
res = (res + k) % i;
return res + 1;
}
\endcode

\h2{the Solution is $O(k \log n)$}

For relatively small $k$ you can come up with a better solution than the above recursive solution for $O(n)$. If $k$ is small, then even it is intuitively clear that the algorithm makes a lot of unnecessary actions: significant changes occur only when there is a capture modulo $n$, and up to this point the algorithm several times just adds to the answer the number of $k$. Accordingly, it is possible to get rid of these unnecessary steps 

Small this occurs when the difficulty lies in the fact that after the removal of these numbers, we will get a task with a smaller $n$, but the starting position is not in the first number, and somewhere in other place. So, calling itself recursively from a task with a new $n$, then we must carefully translate the result into our numbering system of his own.

Also separately it is necessary to analyse the case when $n$ becomes smaller than $k$ --- in this case, the above optimization will degenerate into an infinite loop.

\bf{Implementation} (for convenience, 0-indexing):
\code
int joseph (int n, int k) {
if (n == 1) return 0;
if (k == 1) return n-1;
if (k > n) return (joseph (n-1, k) + k) % n;
int cnt = n / k;
int res = joseph (n - cnt, k);
res -= n % k;
if (res < 0) res += n;
else res += res / (k - 1);
return res;
}
\endcode

Rate \bf{complexity} of the algorithm. We immediately note that the case $n < k$ we have disassembled an old solution that worked in this case $O(k)$. Now consider the algorithm itself. In fact, on each iteration instead of $n$ numbers we get about $n \left( 1 - \frac{1}{k} \right)$ of integers, so the total number $x$ of iterations of the algorithm can be found approximately from the equation:
$$ n \left( 1 - \frac{1}{k} \right) ^ x = 1, $$
logarithmia it, we get:
$$ \ln n + x \ln \left( 1 - \frac{1}{k} \right) = 0, $$
$$ x = - \frac{ \ln n }{ \ln \left( 1 - \frac{1}{k} \right) }, $$
using the decomposition of the logarithm in a Taylor series, we get a rough estimate:
$$ x \approx k \ln n $$

Thus, the asymptotic behavior of the algorithm is $O(k \log n)$.

\h2{Analytical solution for $k=2$}

In this particular case (which was set this task by Joseph Flavius) the problem is solved much easier.

In the case of even $n$ we get that will be crossed out all the even numbers, and then will challenge for $\frac{n}{2}$, then the answer for $n$ will be obtained from the answer for $\frac{n}{2}$ by multiplying by two and subtracting one (by shift position):

$$ J_{2n,2} = 2 J_{n,2} - 1 $$

Similarly, in the case of odd $n$ will be crossed out all the even numbers, then the first number will remain the task for $\frac{n-1}{2}$, and to account for the shift of positions get the second formula:

$$ J_{2n+1,2} = 2 J_{n,2} + 1 $$

In the implementation you can directly use this recursive dependence. It is possible to translate this pattern into another form: $J_{n,2}$ are the sequence of all odd numbers, "presupposes" with units whenever $n$ is a power of two. This can be written as a single formula:

$$ J_{n,2} = 1 + 2 \left( n - 2^{\lfloor \log_2 n \rfloor} \right) $$

\h2{Analytical solution for $k>2$}

Despite the simple appearance of the problem and a large number of articles on this and related tasks, simple analytical representations of the solution of the problem Joseph is still not found. For small $k$ are derived some formulae, but, apparently, they are difficult to apply in practice (see for example Halbeisen, Hungerbuhler "The Josephus Problem" and Odlyzko, Wilf "Functional iteration and the Josephus problem").